"""
Sideguide ë©€í‹°íƒœìŠ¤í¬ ë°ì´í„°ì…‹ ë¡œë”

ë°ì´í„° êµ¬ì¡°:
- data/bbox/Bbox_xxxx/ : ê° í´ë”ë§ˆë‹¤ ì´ë¯¸ì§€ë“¤ + XML íŒŒì¼ (Object Detection)
- data/surface/Surface_xxxx/ : ê° í´ë”ë§ˆë‹¤ ì´ë¯¸ì§€ë“¤ + MASK/ + XML íŒŒì¼ (Surface Segmentation)  
- data/depth/Depth_xxx/ : ê° í´ë”ë§ˆë‹¤ ìŠ¤í…Œë ˆì˜¤ ì´ë¯¸ì§€ë“¤ + Depth_xxx.conf íŒŒì¼ (Depth Estimation)

ì§€ì› ê¸°ëŠ¥:

"""

from __future__ import annotations

import os
import random
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
import logging

import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import numpy as np
import cv2
from PIL import Image

# Albumentations ì™„ì „ ì œê±° - ê¸°ë³¸ ë³€í™˜ë§Œ ì‚¬ìš©

# ìì²´ ëª¨ë“ˆ import
from .annotation_parser import SafeTripAnnotationParser, ImageAnnotation, BoundingBox, Polygon  
from .calibration_parser import SafeTripCalibrationParser, CalibrationData

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SafeTripMultiTaskDataset(Dataset):
    """
    Sideguide ë©€í‹°íƒœìŠ¤í¬ ë°ì´í„°ì…‹
    
    ì§€ì› íƒœìŠ¤í¬:
    1. Object Detection (Bbox) - 29ê°œ í´ë˜ìŠ¤
    2. Surface Segmentation (Polygon) - 6ê°œ í´ë˜ìŠ¤  
    3. Depth Estimation (Stereo) - ì—°ì†ê°’
    
    ë¶€ë¶„ ë¼ë²¨ë§ ì§€ì›: ëª¨ë“  ì´ë¯¸ì§€ì— ëª¨ë“  íƒœìŠ¤í¬ì˜ ë¼ë²¨ì´ ìˆì„ í•„ìš” ì—†ìŒ
    """
    
    def __init__(
        self,
        data_root: Union[str, Path],
        mode: str = "train",
        image_size: Tuple[int, int] = (640, 640),
        augment: bool = True,
        cache_images: bool = False,
        max_samples: Optional[int] = None,
        target_tasks: List[str] = ["bbox", "surface", "depth"]
    ):
        """
        Args:
            data_root: ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ (data/ í´ë”)
            mode: ëª¨ë“œ ('train', 'val', 'test')
            image_size: ëª©í‘œ ì´ë¯¸ì§€ í¬ê¸° (height, width)
            augment: ë°ì´í„° ì¦ê°• ì‚¬ìš© ì—¬ë¶€
            cache_images: ì´ë¯¸ì§€ ìºì‹± ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€)
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            target_tasks: ëŒ€ìƒ íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸
        """
        self.data_root = Path(data_root)
        self.mode = mode
        self.image_size = image_size
        self.augment = augment and mode == "train"
        self.cache_images = cache_images
        self.max_samples = max_samples
        self.target_tasks = target_tasks
        
        # íŒŒì„œ ì´ˆê¸°í™”
        self.annotation_parser = SafeTripAnnotationParser(data_root)
        self.calibration_parser = SafeTripCalibrationParser(data_root)
        
        # í´ë˜ìŠ¤ ì •ë³´
        self.bbox_classes = self.annotation_parser.BBOX_CLASSES
        self.surface_classes = self.annotation_parser.SURFACE_CLASSES
        self.num_bbox_classes = len(self.bbox_classes)
        self.num_surface_classes = len(self.surface_classes)
        
        # ë°ì´í„° ë¡œë“œ
        self._load_data()
        
        # ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì„¤ì •
        self._setup_transforms()
        
        # ì´ë¯¸ì§€ ìºì‹œ
        self.image_cache = {} if cache_images else None
        
        print(f"ğŸ”— ë°ì´í„°ì…‹ ë¡œë“œ: {len(self)} ìƒ˜í”Œ, íƒœìŠ¤í¬: {target_tasks}")  # ê°„ì†Œí™”ëœ ë¡œê·¸
    
    def _load_data(self):
        """ëª¨ë“  ë°ì´í„° ë¡œë“œ ë° ì¸ë±ìŠ¤ ìƒì„±"""
        # ì–´ë…¸í…Œì´ì…˜ ë°ì´í„° ë¡œë“œ
        if "bbox" in self.target_tasks or "surface" in self.target_tasks:
            all_annotations = self.annotation_parser.parse_all_folders()
            self.annotation_data = all_annotations
        else:
            self.annotation_data = {}
        
        # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë“œ  
        if "depth" in self.target_tasks:
            all_calibrations = self.calibration_parser.parse_all_folders()
            self.calibration_data = all_calibrations
        else:
            self.calibration_data = {}
        
        # í†µí•© ìƒ˜í”Œ ì¸ë±ìŠ¤ ìƒì„±
        self.samples = self._create_sample_index()
        
        # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if self.max_samples and len(self.samples) > self.max_samples:
            self.samples = self.samples[:self.max_samples]
        
        # logger.info(f"ì´ {len(self.samples)}ê°œ ìƒ˜í”Œ ë¡œë“œ")  # ë¡œê·¸ ê°„ì†Œí™”
    
    def _create_sample_index(self) -> List[Dict[str, Any]]:
        """í†µí•© ìƒ˜í”Œ ì¸ë±ìŠ¤ ìƒì„±"""
        samples = []
        
        # ì–´ë…¸í…Œì´ì…˜ ê¸°ë°˜ ìƒ˜í”Œë“¤
        for folder_key, annotations in self.annotation_data.items():
            folder_type = "bbox" if folder_key.startswith("bbox_") else "surface"
            folder_name = folder_key.split("_", 1)[1]  # "bbox_Bbox_0640" -> "Bbox_0640"
            
            for annotation in annotations:
                # ì´ë¯¸ì§€ ê²½ë¡œ êµ¬ì„±
                image_path = self.data_root / folder_type / folder_name / annotation.filename
                
                if not image_path.exists():
                    logger.warning(f"ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path}")
                    continue
                
                sample = {
                    'image_path': image_path,
                    'annotation': annotation,
                    'folder_type': folder_type,
                    'folder_name': folder_name,
                    'has_bbox': folder_type == "bbox" and annotation.has_bboxes,
                    'has_surface': folder_type == "surface" and annotation.has_polygons,
                    'has_depth': False,  # ë³„ë„ ì²˜ë¦¬
                    'depth_data': None
                }
                samples.append(sample)
        
        # Depth ê¸°ë°˜ ìƒ˜í”Œë“¤ (ìŠ¤í…Œë ˆì˜¤ ì´ë¯¸ì§€ ìŒ)
        for folder_name, calib_data in self.calibration_data.items():
            for left_img_path, disparity_path in calib_data.image_pairs:
                sample = {
                    'image_path': left_img_path,  # ì¢Œì¸¡ ì´ë¯¸ì§€ë¥¼ ë©”ì¸ìœ¼ë¡œ
                    'disparity_path': disparity_path,  # disparity ë§µ ê²½ë¡œ ì¶”ê°€
                    'annotation': None,
                    'folder_type': 'depth',
                    'folder_name': folder_name,
                    'has_bbox': False,
                    'has_surface': False,
                    'has_depth': True,
                    'depth_data': calib_data
                }
                samples.append(sample)
        
        # ì…”í”Œ (train ëª¨ë“œì—ì„œë§Œ)
        if self.mode == "train":
            random.shuffle(samples)
        
        return samples
    
    def _setup_transforms(self):
        """ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì„¤ì • - ê¸°ë³¸ ë³€í™˜ë§Œ ì‚¬ìš©"""
        # Albumentations ì™„ì „ ì œê±° - ê¸°ë³¸ ë³€í™˜ë§Œ ì‚¬ìš©
        self.transform = self._basic_transform
    
    def _basic_transform(self, image: np.ndarray, **kwargs) -> Dict[str, Any]:
        """ê¸°ë³¸ ì´ë¯¸ì§€ ë³€í™˜ (Albumentations ì—†ì„ ë•Œ)"""
        # ë¦¬ì‚¬ì´ì¦ˆ
        h, w = image.shape[:2]
        target_h, target_w = self.image_size
        
        # ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
        scale = min(target_w / w, target_h / h)
        new_w, new_h = int(w * scale), int(h * scale)
        
        image = cv2.resize(image, (new_w, new_h))
        
        # íŒ¨ë”©
        pad_w = target_w - new_w
        pad_h = target_h - new_h
        top, bottom = pad_h // 2, pad_h - pad_h // 2
        left, right = pad_w // 2, pad_w - pad_w // 2
        
        image = cv2.copyMakeBorder(
            image, top, bottom, left, right, 
            cv2.BORDER_CONSTANT, value=(0, 0, 0)
        )
        
        # ì •ê·œí™” ë° í…ì„œ ë³€í™˜
        image = image.astype(np.float32) / 255.0
        image_tensor = torch.from_numpy(image).permute(2, 0, 1)
        
        return {
            'image': image_tensor,
            'scale': scale,
            'pad': (left, top, right, bottom)
        }
    
    def _load_image(self, image_path: Path) -> np.ndarray:
        """ì´ë¯¸ì§€ ë¡œë“œ (ìºì‹± ì§€ì›)"""
        if self.image_cache is not None and str(image_path) in self.image_cache:
            return self.image_cache[str(image_path)]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        if not image_path.exists():
            raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image_path}")
        
        image = cv2.imread(str(image_path))
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
        
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ìºì‹±
        if self.image_cache is not None:
            self.image_cache[str(image_path)] = image
        
        return image
    
    def _prepare_bbox_targets(self, annotation: ImageAnnotation, scale: float, pad: Tuple[int, int, int, int]) -> Tuple[torch.Tensor, torch.Tensor]:
        """ë°”ìš´ë”© ë°•ìŠ¤ íƒ€ê²Ÿ ì¤€ë¹„ (YOLO í˜•ì‹)"""
        if not annotation.has_bboxes:
            return torch.zeros((0, 4)), torch.zeros((0,), dtype=torch.long)
        
        left_pad, top_pad, right_pad, bottom_pad = pad
        img_h, img_w = annotation.height, annotation.width
        
        bboxes = []
        labels = []
        
        for bbox in annotation.bboxes:
            # ìŠ¤ì¼€ì¼ë§ ë° íŒ¨ë”© ì ìš©
            x1 = (bbox.x1 * scale + left_pad) / self.image_size[1]
            y1 = (bbox.y1 * scale + top_pad) / self.image_size[0]
            x2 = (bbox.x2 * scale + left_pad) / self.image_size[1]
            y2 = (bbox.y2 * scale + top_pad) / self.image_size[0]
            
            # YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (center_x, center_y, width, height)
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2
            width = x2 - x1
            height = y2 - y1
            
            # ìœ íš¨ì„± ê²€ì‚¬
            if width > 0 and height > 0 and 0 <= center_x <= 1 and 0 <= center_y <= 1:
                bboxes.append([center_x, center_y, width, height])
                labels.append(self.annotation_parser.bbox_class_to_idx[bbox.label])
        
        if not bboxes:
            return torch.zeros((0, 4)), torch.zeros((0,), dtype=torch.long)
        
        return torch.tensor(bboxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)
    
    def _prepare_polygon_targets(self, annotation: ImageAnnotation, scale: float, pad: Tuple[int, int, int, int]) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """í´ë¦¬ê³¤ íƒ€ê²Ÿ ì¤€ë¹„ (í”½ì…€ ë§ˆìŠ¤í¬ + í´ë¦¬ê³¤ ì¢Œí‘œ)"""
        if not annotation.has_polygons:
            return torch.zeros((1, *self.image_size), dtype=torch.float32), []
        
        left_pad, top_pad, right_pad, bottom_pad = pad
        
        # í”½ì…€ ë§ˆìŠ¤í¬ ìƒì„±
        mask = np.zeros(self.image_size, dtype=np.uint8)
        polygon_coords = []
        
        for polygon in annotation.polygons:
            # ìŠ¤ì¼€ì¼ë§ ë° íŒ¨ë”© ì ìš©í•˜ì—¬ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            scaled_points = []
            pixel_points = []
            
            for x, y in polygon.points:
                # ì •ê·œí™”ëœ ì¢Œí‘œë¡œ ë³€í™˜
                scaled_x = (x * scale + left_pad) / self.image_size[1]
                scaled_y = (y * scale + top_pad) / self.image_size[0]
                
                # ë²”ìœ„ ì²´í¬
                scaled_x = max(0, min(1, scaled_x))
                scaled_y = max(0, min(1, scaled_y))
                
                scaled_points.extend([scaled_x, scaled_y])
                
                # í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜ (ë§ˆìŠ¤í¬ ìƒì„±ìš©)
                pixel_x = int(scaled_x * self.image_size[1])
                pixel_y = int(scaled_y * self.image_size[0])
                pixel_points.append([pixel_x, pixel_y])
            
            if len(pixel_points) >= 3:  # ìµœì†Œ 3ê°œ í¬ì¸íŠ¸
                # OpenCVë¡œ í´ë¦¬ê³¤ ë§ˆìŠ¤í¬ ìƒì„±
                polygon_array = np.array(pixel_points, dtype=np.int32)
                cv2.fillPoly(mask, [polygon_array], 1)  # ëª¨ë“  surfaceë¥¼ 1ë¡œ ë§ˆí‚¹
                
                # ì •ê·œí™”ëœ ì¢Œí‘œ ì €ì¥
                if len(scaled_points) >= 6:
                    polygon_coords.append(torch.tensor(scaled_points, dtype=torch.float32))
        
        # ë§ˆìŠ¤í¬ë¥¼ í…ì„œë¡œ ë³€í™˜ (1, H, W)
        mask_tensor = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0)
        
        return mask_tensor, polygon_coords
    
    def _prepare_depth_targets(self, sample: Dict[str, Any]) -> Optional[torch.Tensor]:
        """Depth íƒ€ê²Ÿ ì¤€ë¹„ (disparity ë§µ ë¡œë“œ)"""
        if not sample['has_depth']:
            return None
        
        try:
            # sampleì—ì„œ ì§ì ‘ disparity ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (calibration parserì—ì„œ ì´ë¯¸ ë§¤ì¹­ë¨)
            if 'disparity_path' in sample and sample['disparity_path'] != sample['image_path']:
                disparity_path = sample['disparity_path']
            else:
                # fallback: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì°¾ê¸°
                folder_name = sample['folder_name']  # "Depth_001"
                depth_folder = self.data_root / "depth" / folder_name
                
                # ì¢Œì¸¡ ì´ë¯¸ì§€ íŒŒì¼ëª…ì—ì„œ disparity íŒŒì¼ëª… ì¶”ì¶œ
                left_img_path = sample['image_path']
                img_name = left_img_path.stem  # í™•ì¥ì ì œê±°
                
                # íŒŒì¼ëª…ì—ì„œ ì‹ë³„ì ì¶”ì¶œ (ì˜ˆ: ZED1_KSC_001032_L â†’ ZED1_KSC_001032)
                if img_name.endswith('_L'):
                    base_name = img_name[:-2]
                elif img_name.endswith('_left'):
                    base_name = img_name[:-5]
                else:
                    base_name = img_name
                
                # Disparity ë§µ ê²½ë¡œ êµ¬ì„±
                possible_disp_paths = [
                    depth_folder / f"{base_name}_disp.png",
                    depth_folder / f"{base_name}_disp16.png",
                ]
                
                disparity_path = None
                for disp_path in possible_disp_paths:
                    if disp_path.exists():
                        disparity_path = disp_path
                        break
                
                if disparity_path is None:
                    logger.warning(f"Disparity ë§µ ì—†ìŒ: {base_name}")
                    return torch.zeros(self.image_size, dtype=torch.float32)
            
            # Disparity ë§µ ë¡œë“œ (grayscale)
            disparity_image = cv2.imread(str(disparity_path), cv2.IMREAD_UNCHANGED)
            
            if disparity_image is None:
                logger.warning(f"Disparity ë§µ ë¡œë“œ ì‹¤íŒ¨: {disparity_path}")
                return torch.zeros(self.image_size, dtype=torch.float32)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
            if len(disparity_image.shape) == 3:
                disparity_image = cv2.cvtColor(disparity_image, cv2.COLOR_BGR2GRAY)
            
            # í¬ê¸° ì¡°ì •
            disparity_resized = cv2.resize(disparity_image, (self.image_size[1], self.image_size[0]))
            
            # ì •ê·œí™” (0-255 â†’ 0-1)
            if disparity_resized.dtype == np.uint8:
                disparity_normalized = disparity_resized.astype(np.float32) / 255.0
            else:
                disparity_normalized = disparity_resized.astype(np.float32)
                # 16ë¹„íŠ¸ì˜ ê²½ìš° ì ì ˆíˆ ì •ê·œí™”
                if disparity_normalized.max() > 1.0:
                    disparity_normalized = disparity_normalized / disparity_normalized.max()
            
            # í…ì„œë¡œ ë³€í™˜ (1, H, W) - single channel depth map
            disparity_tensor = torch.from_numpy(disparity_normalized).unsqueeze(0)
            
            return disparity_tensor
            
        except Exception as e:
            logger.warning(f"Depth íƒ€ê²Ÿ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return torch.zeros((1, *self.image_size), dtype=torch.float32)
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """ìƒ˜í”Œ ë°˜í™˜"""
        sample = self.samples[idx]
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = self._load_image(sample['image_path'])
            
            # ê¸°ë³¸ ë³€í™˜ ì ìš©
            transform_result = self._basic_transform(image)
            transformed_image = transform_result['image']
            scale = transform_result['scale']
            pad = transform_result['pad']
            
            # íƒ€ê²Ÿ ì¤€ë¹„
            targets = {}
            
            if sample['annotation']:
                # BBox íƒ€ê²Ÿ
                if "bbox" in self.target_tasks and sample['has_bbox']:
                    bbox_coords, bbox_labels = self._prepare_bbox_targets(
                        sample['annotation'], scale, pad
                    )
                    targets['bboxes'] = bbox_coords
                    targets['bbox_labels'] = bbox_labels
                else:
                    targets['bboxes'] = torch.zeros((0, 4))
                    targets['bbox_labels'] = torch.zeros((0,), dtype=torch.long)
                
                # Surface íƒ€ê²Ÿ  
                if "surface" in self.target_tasks and sample['has_surface']:
                    surface_mask, polygon_coords = self._prepare_polygon_targets(
                        sample['annotation'], scale, pad
                    )
                    targets['surface'] = surface_mask
                    targets['polygons'] = polygon_coords
                else:
                    targets['surface'] = torch.zeros((1, *self.image_size), dtype=torch.float32)
                    targets['polygons'] = []
            else:
                # ì–´ë…¸í…Œì´ì…˜ ì—†ìŒ
                targets['bboxes'] = torch.zeros((0, 4))
                targets['bbox_labels'] = torch.zeros((0,), dtype=torch.long)
                targets['surface'] = torch.zeros((1, *self.image_size), dtype=torch.float32)
                targets['polygons'] = []
            
            # Depth íƒ€ê²Ÿ
            if "depth" in self.target_tasks:
                depth_target = self._prepare_depth_targets(sample)
                targets['depth'] = depth_target
            
            return {
                'image': transformed_image,
                'targets': targets,
                'metadata': {
                    'image_path': str(sample['image_path']),
                    'folder_type': sample['folder_type'],
                    'folder_name': sample['folder_name'],
                    'original_size': (image.shape[1], image.shape[0]),  # (width, height)
                    'scale': scale,
                    'pad': pad
                }
            }
            
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ë¡œë“œ ì˜¤ë¥˜ {idx}: {e}")
            # ë¹ˆ ìƒ˜í”Œ ë°˜í™˜
            return {
                'image': torch.zeros((3, *self.image_size)),
                'targets': {
                    'bboxes': torch.zeros((0, 4)),
                    'bbox_labels': torch.zeros((0,), dtype=torch.long),
                    'surface': torch.zeros((1, *self.image_size), dtype=torch.float32),
                    'polygons': [],
                    'depth': None
                },
                'metadata': {
                    'image_path': str(sample['image_path']),
                    'folder_type': sample['folder_type'],
                    'folder_name': sample['folder_name'],
                    'original_size': (640, 640),
                    'scale': 1.0,
                    'pad': (0, 0, 0, 0)
                }
            }
    

    
    def get_class_info(self) -> Dict[str, Any]:
        """í´ë˜ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            'bbox_classes': self.bbox_classes,
            'surface_classes': self.surface_classes,
            'num_bbox_classes': self.num_bbox_classes,
            'num_surface_classes': self.num_surface_classes
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ í†µê³„ ë°˜í™˜"""
        stats = {
            'total_samples': len(self.samples),
            'task_coverage': {
                'bbox': sum(1 for s in self.samples if s['has_bbox']),
                'surface': sum(1 for s in self.samples if s['has_surface']),
                'depth': sum(1 for s in self.samples if s['has_depth'])
            },
            'folder_distribution': {}
        }
        
        # í´ë”ë³„ ë¶„í¬
        for sample in self.samples:
            folder_key = f"{sample['folder_type']}_{sample['folder_name']}"
            if folder_key not in stats['folder_distribution']:
                stats['folder_distribution'][folder_key] = 0
            stats['folder_distribution'][folder_key] += 1
        
        return stats


def multitask_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ë©€í‹°íƒœìŠ¤í¬ ë°°ì¹˜ ì½œë ˆì´ì…˜ í•¨ìˆ˜
    
    ê°€ë³€ ê¸¸ì´ íƒ€ê²Ÿë“¤ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë ˆì´ì…˜
    """
    images = torch.stack([item['image'] for item in batch])
    
    # íƒ€ê²Ÿë“¤ ìˆ˜ì§‘
    batch_targets: Dict[str, Any] = {
        'bboxes': [],
        'bbox_labels': [],
        'surface': [],
        'polygons': [],
        'depth': []
    }
    
    metadata_list = []
    
    for item in batch:
        targets = item['targets']
        
        # BBox íƒ€ê²Ÿ
        batch_targets['bboxes'].append(targets['bboxes'])
        batch_targets['bbox_labels'].append(targets['bbox_labels'])
        
        # Surface íƒ€ê²Ÿ (í”½ì…€ ë§ˆìŠ¤í¬)
        batch_targets['surface'].append(targets['surface'])
        
        # Polygon íƒ€ê²Ÿ (ì¢Œí‘œ)
        batch_targets['polygons'].append(targets['polygons'])
        
        # Depth íƒ€ê²Ÿ
        depth = targets.get('depth')
        batch_targets['depth'].append(depth)
        
        metadata_list.append(item['metadata'])
    
    # Surface ë§ˆìŠ¤í¬ ìŠ¤íƒœí‚¹
    batch_targets['surface'] = torch.stack(batch_targets['surface'])
    
    # Depth íƒ€ê²Ÿ ìŠ¤íƒœí‚¹ (Noneì´ ì•„ë‹Œ ê²ƒë“¤ë§Œ)
    valid_depth_targets = [d for d in batch_targets['depth'] if d is not None]
    if valid_depth_targets:
        # ëª¨ë“  depth íƒ€ê²Ÿì´ ê°™ì€ shapeì¸ì§€ í™•ì¸
        shapes = [d.shape for d in valid_depth_targets]
        if len(set(shapes)) == 1:
            # ê°™ì€ shapeì´ë©´ ìŠ¤íƒ
            depth_indices = [i for i, d in enumerate(batch_targets['depth']) if d is not None]
            stacked_depth = torch.stack(valid_depth_targets)
            batch_targets['depth_tensor'] = stacked_depth
            batch_targets['depth_indices'] = depth_indices
        else:
            # ë‹¤ë¥¸ shapeì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ìœ ì§€
            batch_targets['depth_tensor'] = None
            batch_targets['depth_indices'] = []
    else:
        batch_targets['depth_tensor'] = None
        batch_targets['depth_indices'] = []
    
    return {
        'images': images,
        'targets': batch_targets,
        'metadata': metadata_list
    }


def create_dataloader(
    data_root: Union[str, Path],
    mode: str = "train",
    batch_size: int = 8,
    num_workers: int = 4,
    **dataset_kwargs
) -> DataLoader:
    """ë°ì´í„°ë¡œë” ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    dataset = SafeTripMultiTaskDataset(
        data_root=data_root,
        mode=mode,
        **dataset_kwargs
    )
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(mode == "train"),
        num_workers=num_workers,
        collate_fn=multitask_collate_fn,
        pin_memory=True,
        drop_last=(mode == "train")
    )


def main():
    """í…ŒìŠ¤íŠ¸ ë° ë°ëª¨ ì‹¤í–‰"""
    # ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
    data_root = Path("data")
    
    if not data_root.exists():
        logger.error(f"ë°ì´í„° ë£¨íŠ¸ í´ë” ì—†ìŒ: {data_root}")
        return
    
    # ë°ì´í„°ì…‹ ìƒì„±
    logger.info("ğŸš€ ë©€í‹°íƒœìŠ¤í¬ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    dataset = SafeTripMultiTaskDataset(
        data_root=data_root,
        mode="train",
        image_size=(640, 640),
        augment=False,  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë¹„í™œì„±í™”
        max_samples=100,  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì œí•œ
        target_tasks=["bbox", "surface"]  # depthëŠ” ì œì™¸ (ì´ë¯¸ì§€ ë§¤ì¹­ ì´ìŠˆ)
    )
    
    # í†µê³„ ì¶œë ¥
    stats = dataset.get_statistics()
    class_info = dataset.get_class_info()
    
    print("\n" + "="*80)
    print("ğŸ“Š SafeTrip ë©€í‹°íƒœìŠ¤í¬ ë°ì´í„°ì…‹ í†µê³„")
    print("="*80)
    
    print(f"ğŸ“ ì „ì²´ ìƒ˜í”Œ: {stats['total_samples']}ê°œ")
    print(f"ğŸ“¦ BBox íƒœìŠ¤í¬: {stats['task_coverage']['bbox']}ê°œ")
    print(f"ğŸ” Surface íƒœìŠ¤í¬: {stats['task_coverage']['surface']}ê°œ")
    print(f"ğŸŒŠ Depth íƒœìŠ¤í¬: {stats['task_coverage']['depth']}ê°œ")
    
    print(f"\nğŸ¯ í´ë˜ìŠ¤ ì •ë³´:")
    print(f"  BBox í´ë˜ìŠ¤: {class_info['num_bbox_classes']}ê°œ")
    print(f"  Surface í´ë˜ìŠ¤: {class_info['num_surface_classes']}ê°œ")
    
    # ìƒ˜í”Œ ë°ì´í„° í…ŒìŠ¤íŠ¸
    logger.info("ğŸ” ìƒ˜í”Œ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        # ì²« ë²ˆì§¸ ìƒ˜í”Œ ë¡œë“œ
        sample = dataset[0]
        
        print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° ì •ë³´:")
        print(f"  ì´ë¯¸ì§€ í˜•íƒœ: {sample['image'].shape}")
        print(f"  BBox ê°œìˆ˜: {len(sample['targets']['bboxes'])}")
        print(f"  Polygon ê°œìˆ˜: {len(sample['targets']['polygons'])}")
        print(f"  ì´ë¯¸ì§€ ê²½ë¡œ: {sample['metadata']['image_path']}")
        print(f"  í´ë” íƒ€ì…: {sample['metadata']['folder_type']}")
        
        # ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸
        logger.info("ğŸ”„ ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸ ì¤‘...")
        dataloader = create_dataloader(
            data_root=data_root,
            mode="train",
            batch_size=4,
            num_workers=0,  # ìœˆë„ìš°ì—ì„œ ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ ë°©ì§€
            max_samples=20,
            target_tasks=["bbox", "surface"]
        )
        
        batch = next(iter(dataloader))
        
        print(f"\nğŸ“¦ ë°°ì¹˜ ì •ë³´:")
        print(f"  ì´ë¯¸ì§€ ë°°ì¹˜ í˜•íƒœ: {batch['images'].shape}")
        print(f"  ë°°ì¹˜ í¬ê¸°: {len(batch['metadata'])}")
        print(f"  ì²« ë²ˆì§¸ BBox ê°œìˆ˜: {len(batch['targets']['bboxes'][0])}")
        
        logger.info("âœ… ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise


if __name__ == "__main__":
    main() 